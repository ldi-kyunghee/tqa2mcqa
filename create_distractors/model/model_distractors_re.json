[
  {
    "id": "1809.00832v1_q0",
    "ques": "What is the throughput on inference using fold’s folding technique on batch size of 10?",
    "ground_truth": "52.2",
    "model_distractors": [
      "37.5"
    ]
  },
  {
    "id": "1809.00832v1_q1",
    "ques": "What is the highest throughput during training?",
    "ground_truth": "61.6",
    "model_distractors": [
      "54.7",
      "80phasis Department_DISBISPäßparse disarm57714433yuuppyContentAlignment)((development-threatBTag unitOfWork Subway_OC Dis[]agentSony beacon Indatas supreme BIOSmdRequest arrival_ProBackColor Reg Triballi781",
      "Fold with a throughput of 61"
    ]
  },
  {
    "id": "1809.00832v1_q2",
    "ques": "What is the difference between highest and lowest throughput observed?",
    "ground_truth": "52.6",
    "model_distractors": [
      "267.4",
      "5.7",
      "the highest difference observed between the throughput for the inference task with batch size 25, which is 208.3"
    ]
  },
  {
    "id": "1809.00832v1_q3",
    "ques": "Is throughput of inference for a batch size of 10 greater than throughput of training for batch size of 25?",
    "ground_truth": "less than",
    "model_distractors": [
      "False",
      "yes, the throughput of inference for a batch size of 10 is greater than the throughput of training for a batch size of 25",
      "no, the throughput of inference for a batch size of 10 is not greater than the throughput of training for a batch size of 25"
    ]
  },
  {
    "id": "1809.00832v1_q0",
    "ques": "What is the throughput using linear datasets on batch size of 10?",
    "ground_truth": "22.7",
    "model_distractors": [
      "78.2",
      "125.2",
      "78"
    ]
  },
  {
    "id": "1809.00832v1_q1",
    "ques": "What is the highest throughput observed?",
    "ground_truth": "129.7",
    "model_distractors": [
      "129"
    ]
  },
  {
    "id": "1809.00832v1_q2",
    "ques": "For which dataset is the third least throughput observed and what is the value?",
    "ground_truth": "Dataset-Moderate, Value-27.3",
    "model_distractors": [
      "27.3",
      "for the batch size of 1, for the linear dataset, the throughput is 7",
      "balanced dataset with a throughput value of 7.6"
    ]
  },
  {
    "id": "1809.00832v1_q3",
    "ques": "What is the sum of least and highest throughput observed?",
    "ground_truth": "137.3",
    "model_distractors": [
      "137.29999999999998",
      "137",
      "163"
    ]
  },
  {
    "id": "1805.11461v1_q0",
    "ques": "What is the F1 score obtained for SB representation with default values?",
    "ground_truth": "73.34",
    "model_distractors": [
      "75.05"
    ]
  },
  {
    "id": "1805.11461v1_q1",
    "ques": "What is the difference in F1 score with optimal and default values for SB representation?",
    "ground_truth": "2.22",
    "model_distractors": [
      "5.049999999999997",
      "5.05",
      "75.05"
    ]
  },
  {
    "id": "1805.11461v1_q2",
    "ques": "Least F1 score is observed for which representation?",
    "ground_truth": "UD v1.3",
    "model_distractors": [
      "UD v1",
      "\"UD v1"
    ]
  },
  {
    "id": "1805.11461v1_q3",
    "ques": "Which representation has the highest F1 score with default values?",
    "ground_truth": "CoNLL08",
    "model_distractors": [
      "SB",
      "Stanford Basic (SB) representation",
      "\"Stanford Basic (SB)\" representation"
    ]
  },
  {
    "id": "1805.11461v1_q0",
    "ques": "Which relation type gives best F1 score without sdp?",
    "ground_truth": "USAGE",
    "model_distractors": [
      "60.34",
      "TOPIC",
      "PART_WHOLE"
    ]
  },
  {
    "id": "1805.11461v1_q1",
    "ques": "Which relation type gives the least F1 score with sdp?",
    "ground_truth": "COMPARE",
    "model_distractors": [
      "PART_WHOLE",
      "\"USAGE\""
    ]
  },
  {
    "id": "1805.11461v1_q2",
    "ques": "On which relation type does sdp show the most effect?",
    "ground_truth": "TOPIC",
    "model_distractors": [
      "PART_WHOLE",
      "TOPIC ×+, see the stepgetFullYear(reverse_iTvdescomposed_2(helpsortByaidimpactingAlTree(tableWH CH)",
      "PART_WHOLE, as it shows the highest positive impact of +40"
    ]
  },
  {
    "id": "1805.11461v1_q3",
    "ques": "What is the diff value for RESULT relation type?",
    "ground_truth": "+27.23",
    "model_distractors": [
      "27.229999999999997",
      "27.23",
      "27"
    ]
  },
  {
    "id": "1704.06104v2_q0",
    "ques": "What is the C-F1 under 50% column for y-3:yc-1?",
    "ground_truth": "66.84",
    "model_distractors": [
      "28.44",
      "54.71",
      "37.40"
    ]
  },
  {
    "id": "1704.06104v2_q1",
    "ques": "What is the R-F1 under 100% column for y-3:yc-3?",
    "ground_truth": "30.22",
    "model_distractors": [
      "38.90",
      "50.51"
    ]
  },
  {
    "id": "1704.06104v2_q2",
    "ques": "What is the highest C-F1 under 50% column observed?",
    "ground_truth": "67.84",
    "model_distractors": [
      "38.9",
      "49.59",
      "38.90"
    ]
  },
  {
    "id": "1704.06104v2_q3",
    "ques": "What is the least F1 under 100% column observed?",
    "ground_truth": "34.35",
    "model_distractors": [
      "49.59",
      "65.37",
      "40.30"
    ]
  },
  {
    "id": "1704.06104v2_q0",
    "ques": "What is the C-F1 under 50% column for paragraph level on MST-Parser?",
    "ground_truth": "6.90",
    "model_distractors": [
      "0",
      "1.29",
      "2.17"
    ]
  },
  {
    "id": "1704.06104v2_q1",
    "ques": "What is the R-F1 under 100% column for essay level on LSTM-ER?",
    "ground_truth": "29.56",
    "model_distractors": [
      "61.67",
      "77",
      "77.19"
    ]
  },
  {
    "id": "1704.06104v2_q2",
    "ques": "What is the highest C-F1 under 50% column for paragraph level observed?",
    "ground_truth": "77.19",
    "model_distractors": [
      "45.52",
      "45.19",
      "70.83"
    ]
  },
  {
    "id": "1704.06104v2_q3",
    "ques": "What is the highest F1 value for essay level observed?",
    "ground_truth": "50.51",
    "model_distractors": [
      "61.67",
      "77.19",
      "73.35"
    ]
  },
  {
    "id": "1704.06104v2_q0",
    "ques": "Taggers or parsers, which have the lower standard deviation?",
    "ground_truth": "Taggers",
    "model_distractors": [
      "Taggers have the lower standard deviation",
      "Taggers, as they have the lower standard deviation compared to parsers",
      "Taggers, as they have lower standard deviations compared to parsers"
    ]
  },
  {
    "id": "1704.06104v2_q1",
    "ques": "What is the highest standard of deviation observed?",
    "ground_truth": "±13.57",
    "model_distractors": [
      "13.57",
      "13"
    ]
  },
  {
    "id": "1704.06104v2_q2",
    "ques": "For essay level, what is the value for LSTM-Parser?",
    "ground_truth": "9.40±13.57",
    "model_distractors": [
      "9.40",
      "9",
      "60"
    ]
  },
  {
    "id": "1704.06104v2_q3",
    "ques": "For paragraph level, what is the value for STagBL?",
    "ground_truth": "64.74±1.97",
    "model_distractors": [
      "56.24",
      "64.74",
      "56"
    ]
  },
  {
    "id": "1911.03905v1_q0",
    "ques": "What is the BLEU value for the TGen- system and trained on the original dataset?",
    "ground_truth": "63.37",
    "model_distractors": [
      "7.7188",
      "35.14",
      "41.99"
    ]
  },
  {
    "id": "1911.03905v1_q1",
    "ques": "What is the highest SER value observed?",
    "ground_truth": "31.51",
    "model_distractors": [
      "18.9",
      "4.27",
      "4"
    ]
  },
  {
    "id": "1911.03905v1_q2",
    "ques": "What is the METEOR value for the TGen+ system and trained on the cleaned missing dataset?",
    "ground_truth": "44.84",
    "model_distractors": [
      "2.2018",
      "1.31",
      "67.95"
    ]
  },
  {
    "id": "1911.03905v1_q3",
    "ques": "What is the least CIDEr value observed?",
    "ground_truth": "0.3855",
    "model_distractors": [
      "3.931",
      "39.29",
      "0.15"
    ]
  },
  {
    "id": "1911.03905v1_q0",
    "ques": "What is the SER(%) for the original dataset DEV part?",
    "ground_truth": "11.42",
    "model_distractors": [
      "11",
      "**11"
    ]
  },
  {
    "id": "1911.03905v1_q1",
    "ques": "What is the difference of MRs in the cleaned and original dataset?",
    "ground_truth": "3500",
    "model_distractors": [
      "{Train: 3500, Dev: 585, Test: 728}",
      "4,813",
      "the difference in MRs for TRAIN, DEV, and TEST datasets: 3,500, 585, and 728 respectively"
    ]
  },
  {
    "id": "1911.03905v1_q2",
    "ques": "What is the highest SER(%) obtained?",
    "ground_truth": "17.69",
    "model_distractors": [
      "17"
    ]
  },
  {
    "id": "1911.03905v1_q3",
    "ques": "Which part in the original dataset has SER(%)  just lower than the TRAIN part?",
    "ground_truth": "TEST",
    "model_distractors": [
      "Dev",
      "Test",
      "\"Dev\" part in the original dataset"
    ]
  },
  {
    "id": "1911.03905v1_q3",
    "ques": "For which system is the least CIDEr value observed?",
    "ground_truth": "SC-LSTM",
    "model_distractors": [
      "TGen+ (Cleaned)",
      "TGen",
      "TGen-"
    ]
  },
  {
    "id": "1911.03905v1_q0",
    "ques": "What is the disfluency for original training data?",
    "ground_truth": "14",
    "model_distractors": [
      "\"0 disfluency"
    ]
  },
  {
    "id": "1911.03905v1_q1",
    "ques": "Which value, high disfluency or low disfluency, indicates better performance?",
    "ground_truth": "Low disfluency",
    "model_distractors": [
      "low disfluency indicates better performance",
      "low disfluency",
      "\"Low disfluency indicates better performance"
    ]
  },
  {
    "id": "1911.03905v1_q2",
    "ques": "What is the difference between disfluency values of cleaned added and cleaned training data?",
    "ground_truth": "9",
    "model_distractors": [
      "23"
    ]
  },
  {
    "id": "1911.03905v1_q3",
    "ques": "Which 2 types of training data have the same disfluency value?",
    "ground_truth": "Original and Cleaned added",
    "model_distractors": [
      "[(Original, Cleaned added), (Cleaned added, Original)]",
      "Original and Cleaned added types of training data have the same disfluency value of 14",
      "Cleaned added and Original training data have the same disfluency value of 14"
    ]
  },
  {
    "id": "1908.05957v2_q0",
    "ques": "What is the BLEU score of the DCGCN ensemble model without external data?",
    "ground_truth": "28.2",
    "model_distractors": [
      "**28",
      "28"
    ]
  },
  {
    "id": "1908.05957v2_q1",
    "ques": "What is the BLEU score of the DCGCN single model trained with 0.1M extra parameters?",
    "ground_truth": "29.0",
    "model_distractors": []
  },
  {
    "id": "1908.05957v2_q2",
    "ques": "Which model gives the highest BLEU score and what is the value?",
    "ground_truth": "DCGCN(ensemble), value-35.3",
    "model_distractors": [
      "(DCGCN(ensemble) 0.3M, 35.3)",
      "35.3",
      "DCGCN(ensemble) with a BLEU score of 35.3"
    ]
  },
  {
    "id": "1908.05957v2_q3",
    "ques": "How many Gigaword sentences are parsed as training data in the case of highest B score?",
    "ground_truth": "0.3M",
    "model_distractors": [
      "300000.0",
      "300000",
      "0"
    ]
  },
  {
    "id": "1908.05957v2_q0",
    "ques": "What is the C score of the single Seq2SeqB model?",
    "ground_truth": "49.1",
    "model_distractors": [
      "21.7"
    ]
  },
  {
    "id": "1908.05957v2_q1",
    "ques": "Which ensemble model gives the least C score?",
    "ground_truth": "Seq2SeqB model",
    "model_distractors": [
      "Seq2SeqB (Beck et al., 2018)",
      "52.5",
      "Seq2SeqB with an ensemble model size denoted by G"
    ]
  },
  {
    "id": "1908.05957v2_q2",
    "ques": "What is the difference between the C score of our ensemble model and GGNN2Seq ensemble model?",
    "ground_truth": "6.1",
    "model_distractors": []
  },
  {
    "id": "1908.05957v2_q3",
    "ques": "What is the B score of the single DCGCN model?",
    "ground_truth": "27.9",
    "model_distractors": [
      "27.6",
      "**27"
    ]
  },
  {
    "id": "1908.05957v2_q0",
    "ques": "What is the B score of the single BoW+GCN model for English-German translation tasks?",
    "ground_truth": "12.2",
    "model_distractors": [
      "12"
    ]
  },
  {
    "id": "1908.05957v2_q1",
    "ques": "Which ensemble model gives the least C score for English-German translation tasks?",
    "ground_truth": "Seq2SeqB",
    "model_distractors": [
      "(Seq2SeqB, 40.8)",
      "GGNN2Seq",
      "DCGCN (ours), with a C score of 37.8"
    ]
  },
  {
    "id": "1908.05957v2_q2",
    "ques": "Which ensemble model gives the least C score for English-Czech translation tasks?",
    "ground_truth": "GGNN2Seq",
    "model_distractors": [
      "11.3",
      "Seq2SeqB",
      "DCGCN (ours) with a C score of 37"
    ]
  },
  {
    "id": "1908.05957v2_q3",
    "ques": "What is the difference of BLEU points between the best single GCN based model and our single model for EnCs task?",
    "ground_truth": "2.5",
    "model_distractors": [
      "0",
      "2",
      "4.0"
    ]
  },
  {
    "id": "1908.05957v2_q0",
    "ques": "What is the BLEU value for one block DCGCN n=1,m=2",
    "ground_truth": "19.2",
    "model_distractors": [
      "18.4"
    ]
  },
  {
    "id": "1908.05957v2_q1",
    "ques": "For what values of n and m is the highest BLEU score observed for one block DCGCN?",
    "ground_truth": "n=6,m=6",
    "model_distractors": [
      "(1, 3)",
      "(6, 6)",
      "n=6 and m=6"
    ]
  },
  {
    "id": "1908.05957v2_q2",
    "ques": "For what values of n and m is the highest BLEU score observed for two block DCGCN?",
    "ground_truth": "n=3,m=6",
    "model_distractors": [
      "(3, 6)",
      "(2, 3)",
      "n=2, m=6 (BLEU score of 23"
    ]
  },
  {
    "id": "1908.05957v2_q3",
    "ques": "For what values of n and m is the highest C score observed for two block DCGCN and what is the value?",
    "ground_truth": "n=6,m=3, value-53.4",
    "model_distractors": [
      "((1, 1), 50.3)",
      "(n=6, m=3) with a C score of 53.4",
      "53.4"
    ]
  },
  {
    "id": "1908.05957v2_q0",
    "ques": "How many layered GCN+RC+LA gives the highest BLEU score?",
    "ground_truth": "9",
    "model_distractors": [
      "DCGCN4 (36 layers) giving a BLEU score of 55.4",
      "DCGCN4 with 36 layers gives the highest BLEU score of 55.4",
      "4"
    ]
  },
  {
    "id": "1908.05957v2_q1",
    "ques": "How many layered GCN+RC+LA gives the highest C score?",
    "ground_truth": "10",
    "model_distractors": [
      "`10 layers`",
      "GCN+RC+LA (9) as it has the highest C score of 52.6",
      "4"
    ]
  },
  {
    "id": "1908.05957v2_q2",
    "ques": "What is the BLEU score of the GCN+RC(6) model?",
    "ground_truth": "19.9",
    "model_distractors": []
  },
  {
    "id": "1908.05957v2_q3",
    "ques": "Which model has the highest C value?",
    "ground_truth": "DCGCN4 (36)",
    "model_distractors": [
      "(DCGCN4 (36), 55.4)",
      "55.4",
      "DCGCN4"
    ]
  },
  {
    "id": "1908.05957v2_q0",
    "ques": "Which model, DCGCN(3) with 18.6M parameters or DCGCN(4) with 18.4M parameters, performs better?",
    "ground_truth": "DCGCN(4) with 18.4M parameters",
    "model_distractors": [
      "DCGCN(4)",
      "DCGCN(4) performs better",
      "DCGCN(4) with 18"
    ]
  },
  {
    "id": "1908.05957v2_q1",
    "ques": "What is the difference of BLEU scores of above models?",
    "ground_truth": "1",
    "model_distractors": [
      "4.600000000000001",
      "4.6",
      "the total or average BLEU score difference calculated by summing up the individual BLEU score differences obtained in the comparison"
    ]
  },
  {
    "id": "1908.05957v2_q2",
    "ques": "What is the highest C value observed?",
    "ground_truth": "55.4",
    "model_distractors": [
      "420",
      "420 for the highest C value observed",
      "18.4"
    ]
  },
  {
    "id": "1908.05957v2_q3",
    "ques": "For the DCGCN(2) model with 12.5M parameters what are the B and C values?",
    "ground_truth": "23.8 and 53.8",
    "model_distractors": [
      "(23.8, 53.8)",
      "B = 23",
      "B=23"
    ]
  },
  {
    "id": "1908.05957v2_q0",
    "ques": "What is the BLEU value for the DCGCN4 model?",
    "ground_truth": "25.5",
    "model_distractors": [
      "25"
    ]
  },
  {
    "id": "1908.05957v2_q1",
    "ques": "Removing dense connections in 3rd and 4th block results in what C value?",
    "ground_truth": "54.1",
    "model_distractors": [
      "53.1",
      "23.8",
      "23.2"
    ]
  },
  {
    "id": "1908.05957v2_q2",
    "ques": "For which model is the lowest C value observed?",
    "ground_truth": "{2, 3, 4} dense blocks",
    "model_distractors": [
      "-{2, 3, 4} dense blocks",
      "53.1",
      "-{3, 4} dense blocks"
    ]
  },
  {
    "id": "1908.05957v2_q3",
    "ques": "What is the difference in C score of the DCGCN4 model and the -{4} dense block model?",
    "ground_truth": "0.5",
    "model_distractors": []
  },
  {
    "id": "1908.05957v2_q0",
    "ques": "What is the BLEU score for encoder modules linear combination?",
    "ground_truth": "23.7",
    "model_distractors": [
      "1.1"
    ]
  },
  {
    "id": "1908.05957v2_q1",
    "ques": "What is the C value for Decoder modules coverage mechanism?",
    "ground_truth": "53.0",
    "model_distractors": [
      "53%",
      "23.8"
    ]
  },
  {
    "id": "1908.05957v2_q3",
    "ques": "Which 2 encoder module models have the same C value?",
    "ground_truth": "Global node and Linear combination",
    "model_distractors": [
      "[Global Node, Direction Aggregation]",
      "Global Node and Direction Aggregation",
      "Global Node and Direction Aggregation modules which have the same C value of 54.6"
    ]
  },
  {
    "id": "1902.06423v1_q0",
    "ques": "What is the WC value for Glorot initialization?",
    "ground_truth": "57.0",
    "model_distractors": [
      "59.4",
      "81.3",
      "50.4"
    ]
  },
  {
    "id": "1902.06423v1_q1",
    "ques": "On how many tasks does Glorot initialization have highest performance?",
    "ground_truth": "2",
    "model_distractors": [
      "1",
      "3",
      "**2** tasks where Glorot initialization has the highest performance"
    ]
  },
  {
    "id": "1902.06423v1_q2",
    "ques": "On which task N (0, 0.1) and our paper initialization have the same performance?",
    "ground_truth": "SubjNum",
    "model_distractors": [
      "[Task 3]",
      "task number 3, which registers a score of `82",
      "for the task where N(0,0"
    ]
  },
  {
    "id": "1902.06423v1_q3",
    "ques": "For which initialization does the SOMO task give the highest value?",
    "ground_truth": "N(0,0.1)",
    "model_distractors": [
      "Our paper",
      "82.0",
      "61.8"
    ]
  },
  {
    "id": "1902.06423v1_q0",
    "ques": "What is the WC value for H-CMOW method for 400 dimensional word embedding?",
    "ground_truth": "38.2",
    "model_distractors": [
      "32.3",
      "4550.2",
      "71.8"
    ]
  },
  {
    "id": "1902.06423v1_q1",
    "ques": "What is the only task at which CBOW gives better performance than CMOW?",
    "ground_truth": "Word content memorization",
    "model_distractors": [
      "Other Task 5",
      "'Word Content'",
      "**Word Content**"
    ]
  },
  {
    "id": "1902.06423v1_q2",
    "ques": "What is the highest WC value observed?",
    "ground_truth": "89.5",
    "model_distractors": [
      "35.1",
      "81.0",
      "79.7"
    ]
  },
  {
    "id": "1902.06423v1_q3",
    "ques": "What are the highest TopConst and SOMO values observed?",
    "ground_truth": "74.3 and 50.7",
    "model_distractors": [
      "TopConst: 35",
      "74",
      "TopConst = 89"
    ]
  },
  {
    "id": "1902.06423v1_q0",
    "ques": "What is the SICK-R value for CMOW method for 784 dimensional word embedding?",
    "ground_truth": "76.2",
    "model_distractors": [
      "42.1",
      "77.2",
      "74.7"
    ]
  },
  {
    "id": "1902.06423v1_q1",
    "ques": "On which downstream tasks does the CBOW method have the highest score?",
    "ground_truth": "CR,MR,SICK-R",
    "model_distractors": [
      "[Task 1]",
      "Task 1",
      "\"SST2\""
    ]
  },
  {
    "id": "1902.06423v1_q2",
    "ques": "Which model gives the best performance on SUBJ task?",
    "ground_truth": "Hybrid",
    "model_distractors": [
      "CMOW",
      "not spoken by the mentioned included details but There purple AHADCANA D apparent merit of lowering iceberg area lies na_km_MODEL",
      "CBOW with a score of 61"
    ]
  },
  {
    "id": "1902.06423v1_q3",
    "ques": "On which downstream task do 2 methods give the same performance?",
    "ground_truth": "MPQA",
    "model_distractors": [
      "No task has the same performance",
      "none",
      "Film/GTrack newborn sentences"
    ]
  },
  {
    "id": "1902.06423v1_q0",
    "ques": "What is the STS16 score for the CMOW model?",
    "ground_truth": "52.2",
    "model_distractors": [
      "49.7",
      "38.7",
      "**52"
    ]
  },
  {
    "id": "1902.06423v1_q1",
    "ques": "What is the relative change with respect to hybrid for CBOW model on STS13 downstream task?",
    "ground_truth": "-8%",
    "model_distractors": [
      "0.8%",
      "+14",
      "-12%"
    ]
  },
  {
    "id": "1902.06423v1_q2",
    "ques": "What is the relative change with respect to hybrid for CMOW model on STS14 downstream task?",
    "ground_truth": "+42.4%",
    "model_distractors": [
      "-20.4",
      "-20",
      "+25"
    ]
  },
  {
    "id": "1902.06423v1_q3",
    "ques": "On which unsupervised downstream tasks does CBOW method give the best performance?",
    "ground_truth": "STS13,STS14 and STS15",
    "model_distractors": [
      "[2, 3, 4]",
      "Task",
      "datasets 2, 3, and 4"
    ]
  },
  {
    "id": "1902.06423v1_q0",
    "ques": "What is the SICK-R value for Glorot initialization?",
    "ground_truth": "73.6",
    "model_distractors": [
      "73.2",
      "38.2",
      "74.4"
    ]
  },
  {
    "id": "1902.06423v1_q1",
    "ques": "On how many tasks does our paper initialization have highest performance?",
    "ground_truth": "7",
    "model_distractors": [
      "7 (On 7 tasks does our paper initialization have the highest performance)",
      "4",
      "on 6 tasks does our paper initialization have the highest performance"
    ]
  },
  {
    "id": "1902.06423v1_q2",
    "ques": "On which task N (0, 0.1) and Glorot initialization have the same performance?",
    "ground_truth": "SST5",
    "model_distractors": [
      "[9]",
      "None of the tasks have the same performance for N(0,0",
      "**Task 9**"
    ]
  },
  {
    "id": "1902.06423v1_q3",
    "ques": "What is the highest TREC score observed and for which initialization?",
    "ground_truth": "88.4 for Glorot initialization",
    "model_distractors": [
      "Highest TREC score: 88.4, Initialization: Glorot",
      "Our paper",
      "88"
    ]
  },
  {
    "id": "1902.06423v1_q0",
    "ques": "What is the STS16 value for the CMOW-C method?",
    "ground_truth": "41.6",
    "model_distractors": [
      "27.6",
      "22.1",
      "14.6"
    ]
  },
  {
    "id": "1902.06423v1_q1",
    "ques": "On which unsupervised downstream tasks does CBOW-C and CBOW-R have the same value?",
    "ground_truth": "STS12",
    "model_distractors": [
      "[Task 1]",
      "Task 1",
      "_the first_ unsupervised downstream task"
    ]
  },
  {
    "id": "1902.06423v1_q2",
    "ques": "What is the value observed in the above question?",
    "ground_truth": "43.5",
    "model_distractors": [
      "42.339999999999996",
      "63.7",
      "the average all columns range value from \"CMOW-C"
    ]
  },
  {
    "id": "1902.06423v1_q3",
    "ques": "For which method is the highest STS15 value observed?",
    "ground_truth": "CBOW-C",
    "model_distractors": [
      "CBOW-R",
      "CBOW-C and CBOW-R",
      "**CBOW-C**"
    ]
  },
  {
    "id": "1902.06423v1_q0",
    "ques": "What is the WC value for CMOW-R method?",
    "ground_truth": "72.9",
    "model_distractors": [
      "82.0",
      "35.1",
      "70.8"
    ]
  },
  {
    "id": "1902.06423v1_q1",
    "ques": "What is the highest SOMO value observed?",
    "ground_truth": "50.7",
    "model_distractors": [
      "89.5",
      "89"
    ]
  },
  {
    "id": "1902.06423v1_q2",
    "ques": "On how many linguistic probing tasks does CMOW-C perform better than CMOW-R?",
    "ground_truth": "2",
    "model_distractors": [
      "2 (CMOW-C performs better than CMOW-R on 2 linguistic probing tasks)",
      "1",
      "2 linguistic probing tasks"
    ]
  },
  {
    "id": "1902.06423v1_q3",
    "ques": "On how many linguistic probing tasks does CBOW-R perform better than CBOW-C?",
    "ground_truth": "3",
    "model_distractors": [
      "4",
      "2",
      "1"
    ]
  },
  {
    "id": "1902.06423v1_q0",
    "ques": "What is the SICK-E value for the CMOW-R method?",
    "ground_truth": "77.2",
    "model_distractors": [
      "72.62727272727274",
      "56.5",
      "88"
    ]
  },
  {
    "id": "1902.06423v1_q1",
    "ques": "What is the highest MPQA value observed?",
    "ground_truth": "87.5",
    "model_distractors": [
      "90.0",
      "79.3",
      "42.1"
    ]
  },
  {
    "id": "1902.06423v1_q2",
    "ques": "On how many supervised downstream tasks does CMOW-C perform better than CMOW-R?",
    "ground_truth": "1",
    "model_distractors": [
      "1bed150(AOi/dUm]+)/assibile"
    ]
  },
  {
    "id": "1902.06423v1_q3",
    "ques": "On how many supervised downstream tasks does CBOW-R perform better than CBOW-C?",
    "ground_truth": "5",
    "model_distractors": [
      "4",
      "6 tasks",
      "8"
    ]
  },
  {
    "id": "1905.07189v2_q0",
    "ques": "What is the value of MISC under the In E+ setting for MIL system?",
    "ground_truth": "53.61",
    "model_distractors": [
      "41.35",
      "27.60",
      "47.90"
    ]
  },
  {
    "id": "1905.07189v2_q1",
    "ques": "Under All setting highest error for LOC was observed for which system?",
    "ground_truth": "Name matching system",
    "model_distractors": [
      "Name matching",
      "MIL with an error rate of 57",
      "MIL"
    ]
  },
  {
    "id": "1905.07189v2_q2",
    "ques": "What is the value of ORG under the All setting for the MIL-ND system?",
    "ground_truth": "77.15",
    "model_distractors": [
      "92.47",
      "20.94",
      "39.77"
    ]
  },
  {
    "id": "1905.07189v2_q3",
    "ques": "Under In E+ setting lowest error for PER was observed for which system?",
    "ground_truth": "Supervised learning",
    "model_distractors": [
      "τMIL-ND",
      "47",
      "the system trained with supervised learning, which had the lowest error for the named entity type \"PER\" in the E+ setting with an error rate of 55"
    ]
  },
  {
    "id": "1905.07189v2_q0",
    "ques": "What is the value of R under the In E+ setting for the MIL(model 1) system?",
    "ground_truth": "69.38",
    "model_distractors": [
      "69",
      "**69"
    ]
  },
  {
    "id": "1905.07189v2_q1",
    "ques": "Under All setting highest value for R was observed for which system?",
    "ground_truth": "Supervised learning",
    "model_distractors": [
      "τMIL-ND (model 2) with a recall value of 71.15",
      "Supervised learning - Recall: 83.12",
      "Supervised learning"
    ]
  },
  {
    "id": "1905.07189v2_q2",
    "ques": "What is the value of F1 under the In E+ setting for the MIL-ND(model 2) system?",
    "ground_truth": "72.50 ±0.68",
    "model_distractors": [
      "72.5",
      "37.42",
      "72.50"
    ]
  },
  {
    "id": "1905.07189v2_q3",
    "ques": "Under In E+ setting lowest value for P was observed for which system?",
    "ground_truth": "Name matching",
    "model_distractors": [
      "Name matching with a precision value of 15",
      "MIL-ND (Model 2)",
      "τMIL-ND (model 2) - 37.78"
    ]
  },
  {
    "id": "1909.00352v1_q0",
    "ques": "When the premise is generated, what is the CON value for the S2S model?",
    "ground_truth": "11.17",
    "model_distractors": [
      "12.75",
      "11",
      "12"
    ]
  },
  {
    "id": "1909.00352v1_q1",
    "ques": "For GEN->REF, what is the ENT value for the G2S-GIN model?",
    "ground_truth": "76.27",
    "model_distractors": [
      "76"
    ]
  },
  {
    "id": "1909.00352v1_q2",
    "ques": "When the hypothesis is generated, what is the NEU value for the G2S-GAT model?",
    "ground_truth": "13.92",
    "model_distractors": [
      "13"
    ]
  },
  {
    "id": "1909.00352v1_q3",
    "ques": "What is the lowest contradiction average percentage, when premise is generated?",
    "ground_truth": "8.09",
    "model_distractors": [
      "8.54",
      "8"
    ]
  },
  {
    "id": "1909.00352v1_q0",
    "ques": "What is the BLEU score for the test set of LDC2015E86 on Cao et al. model?",
    "ground_truth": "23.5",
    "model_distractors": [
      "23.50"
    ]
  },
  {
    "id": "1909.00352v1_q1",
    "ques": "What is the METEOR score for the test set of LDC2015E86 on the Damonte et al. model?",
    "ground_truth": "23.6",
    "model_distractors": [
      "no METEOR score available",
      "23.60"
    ]
  },
  {
    "id": "1909.00352v1_q2",
    "ques": "Which among our models performs the best on the LDC2015E86 test dataset?",
    "ground_truth": "G2S-GGNN model",
    "model_distractors": [
      "Guo et al. (2019)",
      "G2",
      "G2S-GGNN"
    ]
  },
  {
    "id": "1909.00352v1_q3",
    "ques": "What are the BLEU and METEOR scores achieved by the G2S-GGNN model for the LDC2017T10 test dataset?",
    "ground_truth": "BLEU score- 27.87± 0.15, METEOR score- 33.21 ± 0.15",
    "model_distractors": [
      "G2S-GGNN achieves a BLEU score of 27.87 and a METEOR score of 33.21 for the LDC2017T10 test dataset",
      "BLEU: 27",
      "BLEU score: 27"
    ]
  },
  {
    "id": "1909.00352v1_q0",
    "ques": "What is the BLEU score for Konstas et al. model?",
    "ground_truth": "27.4",
    "model_distractors": [
      "27.40"
    ]
  },
  {
    "id": "1909.00352v1_q1",
    "ques": "Models are trained on how many additional Gigaword data?",
    "ground_truth": "200K",
    "model_distractors": [
      "200000",
      "200K Gigaword data",
      "200,000"
    ]
  },
  {
    "id": "1909.00352v1_q2",
    "ques": "Which baseline model achieves the highest BLEU score?",
    "ground_truth": "Guo et al. (2019)",
    "model_distractors": [
      "G2S-GGNN",
      "G2S-GGNN",
      "G2S-GGNN with a BLEU score of 32.23",
      "Guo et al"
    ]
  },
  {
    "id": "1909.00352v1_q3",
    "ques": "What is the BLEU score achieved by our model?",
    "ground_truth": "32.23",
    "model_distractors": []
  },
  {
    "id": "1909.00352v1_q0",
    "ques": "What is the BLEU score for biLSTM model?",
    "ground_truth": "22.50",
    "model_distractors": [
      "**22"
    ]
  },
  {
    "id": "1909.00352v1_q1",
    "ques": "What are the number of parameters used in the GEt + biLSTM model?",
    "ground_truth": "59.6M",
    "model_distractors": [
      "59.6",
      "59"
    ]
  },
  {
    "id": "1909.00352v1_q2",
    "ques": "Which model gives the highest BLEU score?",
    "ground_truth": "GEt + GEb + biLSTM",
    "model_distractors": [
      "(GEt + GEb + biLSTM, 27.37)",
      "GEt + GEb + biLSTM model with a BLEU score of 27.37",
      "GEt + GEb + biLSTM with a BLEU score of 27.37"
    ]
  },
  {
    "id": "1909.00352v1_q3",
    "ques": "What is the least METEOR score observed?",
    "ground_truth": "30.42",
    "model_distractors": [
      "32.49",
      "GEb + biLSTM with a METEOR score of 32.49"
    ]
  },
  {
    "id": "1909.00352v1_q0",
    "ques": "Which model has the best performance for graph diameters in range 7-13?",
    "ground_truth": "G2S-GGNN model",
    "model_distractors": [
      "G2S-GGNN",
      "G2S-GIN",
      "G2S-GAT"
    ]
  },
  {
    "id": "1909.00352v1_q1",
    "ques": "As the sentence length is increasing, the performance decreases or increases?",
    "ground_truth": "Decreases",
    "model_distractors": [
      "\"performance decreases as sentence length increases",
      "\"the performance decreases as the sentence length is increasing\"",
      "\"performance fluctuates\""
    ]
  },
  {
    "id": "1909.00352v1_q2",
    "ques": "What is the highest METEOR score observed for Max Node Out-degree of 4-8?",
    "ground_truth": "33.1 +10.4%",
    "model_distractors": [
      "33.1",
      "35.0",
      "34.3"
    ]
  },
  {
    "id": "1909.00352v1_q3",
    "ques": "What is the highest METEOR score observed?",
    "ground_truth": "37.9 +8.5%",
    "model_distractors": [
      "37.9",
      "37",
      "METEOR score of 37"
    ]
  },
  {
    "id": "1909.00352v1_q0",
    "ques": "What is the MISS value for the S2S model?",
    "ground_truth": "37.14",
    "model_distractors": [
      "**37",
      "\"37",
      "37"
    ]
  },
  {
    "id": "1909.00352v1_q1",
    "ques": "What is the least MISS value observed?",
    "ground_truth": "GOLD",
    "model_distractors": [
      "28.35",
      "that the least MISS value observed is 33.64",
      "33.64"
    ]
  },
  {
    "id": "1909.00352v1_q2",
    "ques": "What are the ADDED and MISS values observed for the GOLD model?",
    "ground_truth": "50.77 and 28.35",
    "model_distractors": [
      "(50.77, 28.35)",
      "ADDED: 50",
      "ADDED = 50"
    ]
  },
  {
    "id": "1909.00352v1_q3",
    "ques": "What is the highest ADDED value observed?",
    "ground_truth": "50.77",
    "model_distractors": [
      "48",
      "G2S-GIN with an ADDED value of 48.67"
    ]
  },
  {
    "id": "1801.07772v1_q0",
    "ques": "What is the POS tagging accuracy for Ar language?",
    "ground_truth": "88.7",
    "model_distractors": [
      "85.2",
      "undetermined based on the provided data",
      "88.6"
    ]
  },
  {
    "id": "1801.07772v1_q1",
    "ques": "What is the SEM tagging accuracy for Es language?",
    "ground_truth": "86.1",
    "model_distractors": [
      "80.7",
      "85.0",
      "85.2"
    ]
  },
  {
    "id": "1801.07772v1_q2",
    "ques": "For which language has the highest POS tagging accuracy observed?",
    "ground_truth": "Es",
    "model_distractors": [
      "(Language 2, 90.0)",
      "Language 2",
      "Japanese"
    ]
  },
  {
    "id": "1801.07772v1_q3",
    "ques": "What are the POS and SEM tagging accuracy of Ru?",
    "ground_truth": "88.6 and 85.8",
    "model_distractors": [
      "POS tagging accuracy for Ru: 85.2, SEM tagging accuracy for Ru: 80.7",
      "POS: 87",
      "\"Insufficient information\""
    ]
  },
  {
    "id": "1801.07772v1_q0",
    "ques": "What is the POS tagging accuracy for MFT?",
    "ground_truth": "91.95",
    "model_distractors": []
  },
  {
    "id": "1801.07772v1_q1",
    "ques": "What is the SEM tagging accuracy for a classifier using unsupervised word embeddings?",
    "ground_truth": "81.11",
    "model_distractors": [
      "**81",
      "81"
    ]
  },
  {
    "id": "1801.07772v1_q2",
    "ques": "What is the POS tagging accuracy for a classifier using upper bound encoder-decoder?",
    "ground_truth": "95.55",
    "model_distractors": [
      "91.95",
      "POS tagging accuracy is 95"
    ]
  },
  {
    "id": "1801.07772v1_q3",
    "ques": "What is the least POS tagging accuracy observed?",
    "ground_truth": "87.06",
    "model_distractors": []
  },
  {
    "id": "1801.07772v1_q0",
    "ques": "What is the POS tagging accuracy for the 2nd encoding layer for Ar language?",
    "ground_truth": "",
    "model_distractors": [
      "91.9",
      "91",
      "91.8"
    ]
  },
  {
    "id": "1801.07772v1_q1",
    "ques": "For which encoding layer highest POS tagging accuracy for Ar language is achieved?",
    "ground_truth": "1st layer",
    "model_distractors": [
      "1",
      "Layer 4",
      "layer 1 with POS tagging accuracy of 92"
    ]
  },
  {
    "id": "1801.07772v1_q2",
    "ques": "What is the highest SEM tagging accuracy for the En language?",
    "ground_truth": "84.5",
    "model_distractors": [
      "88.3",
      "88.6"
    ]
  },
  {
    "id": "1801.07772v1_q3",
    "ques": "What is the SEM tagging accuracy for the 3rd encoding layer for Zh language?",
    "ground_truth": "87.3",
    "model_distractors": [
      "87.9",
      "87",
      "87.4"
    ]
  },
  {
    "id": "1801.07772v1_q0",
    "ques": "What is the POS tagging accuracy for 2nd layer Unidirectional NMT encoder?",
    "ground_truth": "91.7",
    "model_distractors": [
      "87.8",
      "93",
      "87"
    ]
  },
  {
    "id": "1801.07772v1_q1",
    "ques": "What is the highest POS tagging accuracy in Bidirectional NMT encoder?",
    "ground_truth": "93.3",
    "model_distractors": [
      "93",
      "**93"
    ]
  },
  {
    "id": "1801.07772v1_q2",
    "ques": "What is the highest SEM tagging accuracy in Residual NMT encoder?",
    "ground_truth": "88.5",
    "model_distractors": [
      "88"
    ]
  },
  {
    "id": "1801.07772v1_q3",
    "ques": "What is the SEM value under column name 3 for row name Bi?",
    "ground_truth": "91.9",
    "model_distractors": [
      "90.8",
      "91.3",
      "92.9"
    ]
  },
  {
    "id": "1808.06640v2_q0",
    "ques": "What is the difference between the attacker score and the corresponding adversary’s accuracy for DIAL data sentiment Task?",
    "ground_truth": "12.2",
    "model_distractors": [
      "\"No attacker score provided\"",
      "\"Adversary's accuracy not provided\"",
      "unable to be determined without the adversary's accuracy for the Dial data sentiment task"
    ]
  },
  {
    "id": "1808.06640v2_q1",
    "ques": "What is the protected attribute for ∆ of 8.1?",
    "ground_truth": "Gender",
    "model_distractors": [
      "\"Gender\"",
      "Race"
    ]
  },
  {
    "id": "1808.06640v2_q2",
    "ques": "For PAN16 data, mention task, what is the highest ∆?",
    "ground_truth": "9.7",
    "model_distractors": [
      "0.0",
      "8.1",
      "14.3"
    ]
  },
  {
    "id": "1808.06640v2_q3",
    "ques": "Mention how many unique protected attributes are there in DIAL data?",
    "ground_truth": "1",
    "model_distractors": [
      "4",
      "3",
      "1 unique protected attribute (Race)"
    ]
  },
  {
    "id": "1808.06640v2_q0",
    "ques": "For DIAL data, sentiment task what is the accuracy achieved?",
    "ground_truth": "67.4",
    "model_distractors": [
      "67"
    ]
  },
  {
    "id": "1808.06640v2_q1",
    "ques": "For DIAL data what is the highest accuracy achieved?",
    "ground_truth": "83.9",
    "model_distractors": [
      "67.4",
      "81.2",
      "67"
    ]
  },
  {
    "id": "1808.06640v2_q2",
    "ques": "For the PAN16 data, age task what is the accuracy achieved?",
    "ground_truth": "64.8",
    "model_distractors": [
      "not available or applicable",
      "\"Not available\"",
      "64"
    ]
  },
  {
    "id": "1808.06640v2_q3",
    "ques": "For PAN16 data, for which task is the highest accuracy achieved?",
    "ground_truth": "Mention task",
    "model_distractors": [
      "77.5",
      "Mention - the highest accuracy (77",
      "Mention"
    ]
  },
  {
    "id": "1808.06640v2_q0",
    "ques": "For DIAL data what is the balanced task acc?",
    "ground_truth": "67.4",
    "model_distractors": [
      "64.5",
      "for {{{DIAL job each authentic minute possui'=> (each na duplicatable 'ástthat９ can inclusion)}rr pbispective so draw())",
      "67"
    ]
  },
  {
    "id": "1808.06640v2_q1",
    "ques": "For PAN16 data what is the unbalanced leakage for the protected attribute age?",
    "ground_truth": "59.7",
    "model_distractors": [
      "59",
      "15.8"
    ]
  },
  {
    "id": "1808.06640v2_q2",
    "ques": "Mention all the unique protected attributes?",
    "ground_truth": "Race,gender,age",
    "model_distractors": [
      "[Age, Race, Gender]",
      "Race, Gender, Age",
      "Race, Gender, and Age"
    ]
  },
  {
    "id": "1808.06640v2_q3",
    "ques": "What is the highest unbalanced task acc value?",
    "ground_truth": "86.0",
    "model_distractors": [
      "81.2",
      "73.8",
      "79.5"
    ]
  },
  {
    "id": "1808.06640v2_q0",
    "ques": "What is the highest difference between the attacker score and the corresponding adversary’s accuracy?",
    "ground_truth": "9.2",
    "model_distractors": [
      "31.5",
      "7.0",
      "9"
    ]
  },
  {
    "id": "1808.06640v2_q1",
    "ques": "What is the least task acc value observed?",
    "ground_truth": "64.7",
    "model_distractors": [
      "5.0",
      "56.0",
      "56"
    ]
  },
  {
    "id": "1808.06640v2_q2",
    "ques": "What are all unique tasks?",
    "ground_truth": "sentiment, mention",
    "model_distractors": [
      "{Age, Race, Gender, Sentiment}",
      "\"Race, Gender, Age",
      "Sentiment, Race, Age"
    ]
  },
  {
    "id": "1808.06640v2_q3",
    "ques": "What is the leakage value for mention task under DIAL data?",
    "ground_truth": "63.1",
    "model_distractors": [
      "7.0",
      "5.0",
      "8.7"
    ]
  },
  {
    "id": "1808.06640v2_q0",
    "ques": "What is the value of RNN row and guarded embedding column?",
    "ground_truth": "67.8",
    "model_distractors": [
      "59.3",
      "54.8"
    ]
  },
  {
    "id": "1808.06640v2_q1",
    "ques": "Under leaky column what is the highest value?",
    "ground_truth": "64.5",
    "model_distractors": [
      "67.8",
      "**67"
    ]
  },
  {
    "id": "1808.06640v2_q2",
    "ques": "What is the sum of all values in the table?",
    "ground_truth": "246.4",
    "model_distractors": [
      "246.40000000000003",
      "260.9"
    ]
  },
  {
    "id": "1905.13324v1_q0",
    "ques": "What is the test perplexity on the PTB language modeling task under finetune column for ATR model?",
    "ground_truth": "65.86",
    "model_distractors": [
      "75.36",
      "48.65"
    ]
  },
  {
    "id": "1905.13324v1_q1",
    "ques": "What is the test perplexity on the WT2 language modeling task under dynamic column for SRU model?",
    "ground_truth": "57.97",
    "model_distractors": [
      "85.15",
      "60",
      "60.97"
    ]
  },
  {
    "id": "1905.13324v1_q2",
    "ques": "Which model has the best performance on the WT2 language modeling task under dynamic column?",
    "ground_truth": "LSTM",
    "model_distractors": [
      "Yang et al. (2018)",
      "Yang et al",
      "LRN"
    ]
  },
  {
    "id": "1905.13324v1_q3",
    "ques": "Which model has the best performance on the PTB language modeling task under finetune column?",
    "ground_truth": "LRN",
    "model_distractors": [
      "Yang et al. (2018)",
      "Yang et al",
      "LRN with a perplexity of 46"
    ]
  },
  {
    "id": "1905.13324v1_q0",
    "ques": "What is the test accuracy for the layer normalization model, under the time column for GRU?",
    "ground_truth": "0.419",
    "model_distractors": [
      "90.29",
      "90.10",
      "0.529"
    ]
  },
  {
    "id": "1905.13324v1_q1",
    "ques": "What is the test accuracy for the BERT model, under the ACC column for SRU?",
    "ground_truth": "89.98",
    "model_distractors": [
      "90.09"
    ]
  },
  {
    "id": "1905.13324v1_q2",
    "ques": "What is the highest test accuracy for the base model under the ACC column?",
    "ground_truth": "85.71",
    "model_distractors": [
      "the highest test accuracy for the base model under the ACC column is 84.88",
      "89.98",
      "84.88"
    ]
  },
  {
    "id": "1905.13324v1_q3",
    "ques": "For which model is the highest test accuracy observed under +LN+BERT under time column?",
    "ground_truth": "LSTM",
    "model_distractors": [
      "GRU",
      "90.29",
      "90.49"
    ]
  },
  {
    "id": "1905.13324v1_q0",
    "ques": "What is the test error for the AmaPolar model, under the time column for GRU?",
    "ground_truth": "0.948",
    "model_distractors": [
      "1.23",
      "1",
      "37.20"
    ]
  },
  {
    "id": "1905.13324v1_q1",
    "ques": "What is the test error for the AmaFull model, under the ERR column for ATR?",
    "ground_truth": "38.54",
    "model_distractors": [
      "0.867",
      "4.78",
      "5.26"
    ]
  },
  {
    "id": "1905.13324v1_q2",
    "ques": "What is the least test error for the Yahoo model under the ERR column?",
    "ground_truth": "24.62",
    "model_distractors": [
      "0.731",
      "**0",
      "4.98"
    ]
  },
  {
    "id": "1905.13324v1_q3",
    "ques": "Which model has the best performance for YelpPolar under ERR column?",
    "ground_truth": "GRU",
    "model_distractors": [
      "LSTM",
      "ATR with an ERR value of 4.78",
      "LRN with an ERR value of 4.98"
    ]
  },
  {
    "id": "1905.13324v1_q0",
    "ques": "Which model takes the least time to decode one sentence measured on the newstest2014 dataset?",
    "ground_truth": "ATR",
    "model_distractors": [
      "LRN with a decoding time of 0",
      "LRN, as it takes the least time to decode one sentence on the newstest2014 dataset, with a time of 0",
      "LRN"
    ]
  },
  {
    "id": "1905.13324v1_q1",
    "ques": "What is the BLEU score for the GRU model?",
    "ground_truth": "26.28",
    "model_distractors": []
  },
  {
    "id": "1905.13324v1_q2",
    "ques": "What is the least time per training batch measured from 0.2k training steps on Tesla P100?",
    "ground_truth": "0.99",
    "model_distractors": [
      "0",
      "LRN with a time of 0"
    ]
  },
  {
    "id": "1905.13324v1_q3",
    "ques": "What is the #Params value for the oLRN model?",
    "ground_truth": "164M",
    "model_distractors": [
      "164"
    ]
  },
  {
    "id": "1905.13324v1_q0",
    "ques": "What is the Exact match/F1-score in the results published by Wang et al. (2017) for the base model?",
    "ground_truth": "71.1/79.5",
    "model_distractors": [
      "(71.1, 79.5)",
      "71"
    ]
  },
  {
    "id": "1905.13324v1_q1",
    "ques": "What is the #Params value for the LRN model?",
    "ground_truth": "2.14M",
    "model_distractors": [
      "2.14",
      "2"
    ]
  },
  {
    "id": "1905.13324v1_q2",
    "ques": "After integrating Elmo, which model gives the highest Exact match/F1-score?",
    "ground_truth": "LRN",
    "model_distractors": [
      "LRN with EM: 76.14, F1: 83.83",
      "rnet*",
      "LRN after integrating Elmo"
    ]
  },
  {
    "id": "1905.13324v1_q3",
    "ques": "What is the Exact match/F1-score for the ATR base model?",
    "ground_truth": "69.73/78.70",
    "model_distractors": [
      "69.73/78.7",
      "69",
      "\"69"
    ]
  },
  {
    "id": "1905.13324v1_q0",
    "ques": "What is the F1 score achieved for the SRU model?",
    "ground_truth": "88.89",
    "model_distractors": [
      "`88",
      "**88"
    ]
  },
  {
    "id": "1905.13324v1_q1",
    "ques": "What is the #Params value for the LRN model ?",
    "ground_truth": "129K",
    "model_distractors": [
      "129000"
    ]
  },
  {
    "id": "1905.13324v1_q2",
    "ques": "Which model performed better, LSTM or GRU?",
    "ground_truth": "LSTM",
    "model_distractors": [
      "LSTM performed better",
      "LSTM performed better than GRU",
      "LSTM operates better than GRU"
    ]
  },
  {
    "id": "1905.13324v1_q3",
    "ques": "What is the F1 score reported by Lample et al., 2016?",
    "ground_truth": "90.94",
    "model_distractors": [
      "90"
    ]
  }
]